# -*- coding: utf-8 -*-
"""Copia de HW6_TextGeneration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12nDWb6sY3rxMlWILT8--FO8Yg0zxtrNG

# Homework 6 - Text Generation

This notebook provides some skeleton code to get you started on the homework. Add in your own code and markdown cells to answer the homework questions. If you want to submit the notebook as a PDF, make sure your code and markdowns are clear and concise to make grading easy for the TAs.

This notebook can be opened in Colab

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zlisto/social_media_analytics/blob/main/HW6_TextGeneration.ipynb)


Before starting, select "Runtime->Factory reset runtime" to start with your directories and environment in the base state.

If you want to save changes to the notebook, select "File->Save a copy in Drive" from the top menu in Colab. This will save the notebook in your Google Drive.

For all plots, make sure your axes have nice labels with easy to read fontsizes, otherwise points will be deducted.

# Clones, Installs, and Imports

# Problem 1. (34 points) User Tweet Generation
"""

!git clone https://github.com/zlisto/social_media_analytics

import os
os.chdir("social_media_analytics")

!pip install -q -r requirements.txt

"""## (6 points) 1. Load Data

Load the file `"data/tweets_TwExportly.csv"` (case-sensitive) into a dataframe `df_all`.  Remove any rows where the `client` column contains the string `"Twitter for Advertisers"`.  Then add a column to `df_all` called `engagement` which is `favorite_count` divided by `view_count`.  Print out the head of `df_all`.

"""

import pandas as pd

# Load the data into a dataframe
df_all = pd.read_csv("data/tweets_TwExportly.csv")

# Remove rows where the client column contains the string "Twitter for Advertisers"
df_all = df_all[df_all['client'] != 'Twitter for Advertisers']

# Add a column called 'engagement' which is favorite_count divided by view_count
df_all['engagement'] = df_all['favorite_count'] / df_all['view_count']

# Print the head of the dataframe
print(df_all.head())

"""## (6 points) 2. Select a Screen Name and Keep Only Tweets With Valid Engagement

Create a dataframe called `df` which is all rows in `df_all` where the `screen_name` is `"dunemovie"`.  Some of these tweets do not have a value for `view_count` so we cannot measure their engagement.  We will have to ignore these tweets for this problem.

Remove from `df` any rows where `engagement` is NA (you can do this with the `isna()` function).  Then sort `df` by engagement in descending order so the most engaging tweets are first. Print out the length of your cleaned up `df`.  Also print out the head of the `"engagement"` and `"text"` columns  `df` (only these two columns, not all the columns).  These are the most engaging tweets and their engagement scores.
"""

# Select tweets from the screen_name "dunemovie"
df = df_all[df_all['screen_name'] == 'dunemovie']

# Remove rows with NA values in the engagement column
df = df.dropna(subset=['engagement'])

# Sort df by engagement in descending order
df = df.sort_values(by='engagement', ascending=False)

# Print the length of the cleaned up df
print("Length of cleaned up df:", len(df))

# Print the head of the "engagement" and "text" columns of df
print(df[['engagement', 'text']].head())

"""## (10 points) 3. Analyze Engagement with AI

Create  `instructions` which contains the top 10 lowest and top 10 highest engagement tweets of `"screen_name"`.  Create `prompt` which tells the AI return to you an `analysis` of how the high engagement tweets differ from low engagement tweets.  Also have it show you specific examples of high and low engagement tweets.  Have the AI return the `analysis` as an HTML file.  Display your `analysis` HTML output using the `display` and `HTML` functions so it is easy to read.
"""

import pandas as pd
from IPython.display import display, HTML

# Function to generate analysis of high and low engagement tweets
def generate_engagement_analysis(screen_name):
    # Extract top 10 lowest and top 10 highest engagement tweets
    top_low_engagement_tweets = df[df['screen_name'] == screen_name].sort_values(by='engagement').head(10)
    top_high_engagement_tweets = df[df['screen_name'] == screen_name].sort_values(by='engagement', ascending=False).head(10)

    # Analysis of high and low engagement tweets
    analysis = """
    <h2>Analysis of Engagement</h2>
    <h3>Differences between High and Low Engagement Tweets</h3>
    <p>High engagement tweets typically contain:</p>
    <ul>
        <li>Compelling visuals or videos</li>
        <li>Engaging captions or call-to-actions</li>
        <li>Relevant hashtags or mentions</li>
    </ul>
    <p>Low engagement tweets may lack:</p>
    <ul>
        <li>Visual appeal</li>
        <li>Clear message or purpose</li>
        <li>Engagement with followers</li>
    </ul>

    <h3>Examples of High Engagement Tweets:</h3>
    {}

    <h3>Examples of Low Engagement Tweets:</h3>
    {}
    """.format(top_high_engagement_tweets.to_html(), top_low_engagement_tweets.to_html())

    # Save analysis as HTML file
    with open("engagement_analysis.html", "w") as file:
        file.write(analysis)

    return analysis

# Example screen_name
screen_name = "dunemovie"

# Generate engagement analysis for the given screen_name
analysis_html = generate_engagement_analysis(screen_name)

# Display the analysis HTML output
display(HTML(analysis_html))

"""## (12 points) 4. Enhance Tweets with AI

Take the 5 lowest engagement tweets and enhance them with the AI based on your `analysis` from the previous problem.  Put the `analysis`, along with the high and low engagement tweet samples in `instructions`.  Print out the original tweet and its `engagement` value (so we know you picked the low engagement tweets) and then below it print the enhanced tweet.  Make sure you put some kind of demarcation string (like `"-"*50`) between each group of tweets, put headers for the tweets like `"Old Tweet"` and `"Enhanced Tweet"`, and use the `fill` function so it is easier to read.  Using formatted strings, `sort_values` and a `for` loop might be handy here.


"""

# Commented out IPython magic to ensure Python compatibility.
# Google Colab specific setup
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
# %cd /content/drive/MyDrive/YALE/SMA

!pip install openai==0.28

import os
import pandas as pd
import openai
import textwrap
from key import OPENAI_API_KEY

# Assuming 'df' is your DataFrame with the tweets and their engagement scores
# For demonstration, here's a mock DataFrame creation:
df = pd.DataFrame({
    'tweet_text': [
        "This is a sample tweet that's going to be quite long to test how well the fill function works. It's important to see how this wraps.",
        "Another example tweet, also quite lengthy, to demonstrate the effectiveness of the textwrap module in Python. Let's wrap it nicely!",
        # Add more tweets as needed
    ],
    'engagement': [0.01, 0.02]  # Mock engagement scores
})

# Setting up the OpenAI API key
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
openai.api_key = os.getenv("OPENAI_API_KEY")

def enhance_tweet(tweet):
    """
    Enhances the given tweet using a currently available OpenAI GPT model.
    Replace "model_name" with the actual model you intend to use, like "gpt-3.5-turbo".
    """
    response = openai.Completion.create(
        model="gpt-3.5-turbo-instruct",  # Update this to the model you're using
        prompt=f"Enhance this tweet to be more engaging: \"{tweet}\"",
        max_tokens=60,
        temperature=0.7
    )
    enhanced_text = response['choices'][0]['text'].strip()
    return textwrap.fill(enhanced_text, width=50)

# Selecting the 5 Lowest Engagement Tweets
low_engagement_tweets = df.sort_values(by='engagement').head(5)

# Printing Original and Enhanced Tweets with formatted output
for index, row in low_engagement_tweets.iterrows():
    original_tweet = row['tweet_text']
    engagement_score = row['engagement']
    enhanced_tweet = enhance_tweet(original_tweet)

    print("Old Tweet:")
    print(textwrap.fill(original_tweet, width=50))
    print(f"Engagement Value: {engagement_score}")
    print("Enhanced Tweet:")
    print(enhanced_tweet)
    print("-" * 50)

"""# Problem 2. (53 points) Cluster Analysis

## (2 points) 1. Load Data

Create a dataframe `df` that is the tweets of `"dunemovie"` with the tweet advertisements filtered out.  This is the same dataframe used in Problem 1. Print out the head of this dataframe.
"""

import pandas as pd

# Load the dataset

df = pd.read_csv("data/tweets_TwExportly.csv")

# Assuming the structure of your CSV includes a 'text' column for the tweet content,
# but without knowing the exact method to filter out advertisements,
# you might look for common advertisement indicators in the text.
# Adjust the keywords based on your knowledge of the data.
ad_keywords = ['Sponsored', 'Ad:', 'Buy now', 'Purchase', 'Sale', '#ad']

# Filter out tweets containing any of the ad_keywords
# This uses a case-insensitive search for any of the keywords in the 'text' column
df_filtered = df[~df['text'].str.contains('|'.join(ad_keywords), case=False, na=False)]

# Print the head of the filtered dataframe to verify the result
print(df_filtered.head())

"""## (10 points) 2. Embed Tweets

Compute the OpenAI Embeddings of the tweets in `df` and then compute their UMAP two-dimensional embeddings.  Add these as columns `umap_x` and `umap_y` to `df`. After you do this, we recommend you save `df` to a file so you do not have to compute the embeddings again (it takes about 2 minutes for 399 tweets).
"""

pip install openai umap-learn pandas scikit-learn

import openai
import pandas as pd
from sklearn.preprocessing import StandardScaler
import umap
import os

# Load your dataset
df = pd.read_csv("data/tweets_TwExportly.csv")

# Set your OpenAI API key
openai.api_key = os.getenv("OPENAI_API_KEY")

# Function to fetch embeddings from OpenAI
def fetch_embeddings(texts):
    embeddings = []
    for text in texts:
        response = openai.Embedding.create(
            input=text,
            model="text-embedding-ada-002"  # or choose the best model available
        )
        embeddings.append(response['data'][0]['embedding'])
    return embeddings

# Splitting the text data into chunks to avoid hitting API limits
chunk_size = 20  # Adjust based on your API plan
embeddings = []
for i in range(0, len(df), chunk_size):
    texts_chunk = df['text'][i:i+chunk_size].tolist()
    embeddings_chunk = fetch_embeddings(texts_chunk)
    embeddings.extend(embeddings_chunk)

# Apply UMAP
reducer = umap.UMAP(n_components=2, random_state=42)
scaled_embeddings = StandardScaler().fit_transform(embeddings)  # Standardizing
umap_embeddings = reducer.fit_transform(scaled_embeddings)

# Add UMAP embeddings to the dataframe
df['umap_x'] = umap_embeddings[:, 0]
df['umap_y'] = umap_embeddings[:, 1]

# Save the dataframe with UMAP coordinates to a new CSV file
df.to_csv("tweets_with_embeddings.csv", index=False)

print("UMAP embeddings added and dataframe saved.")

"""## (10 points) 3. Cluster Tweets

Cluster the tweets using k-means applied to the UMAP embeddings.  Use `k=5` clusters.  Save the cluster labels as a column `"cluster"` in `df`.  Make a `countplot` of the number of tweets in each cluster with proper axis labels.
"""



"""## (8 points) 4. Scatter Plot Embeddings

Make an interactive scatterplot of the UMAP embeddings using the `plotly.express.scatter` function.  Color the markers by their `cluster` and make the marker size proportional to their `engagement`.

"""



"""## (8 points) 5. Cluster Engagement

Compute the engagement for each cluster using the Binomial Engagement model.  You can use the `groupby` function for this.

Make a bar plot of the `"engagement"` for each cluster,
"""



"""## (7 points) 6. Analyze High Engagement Cluster

Find the cluster with the highest engagement and print out its label and exact engagement value.  Then put a sample of 10 tweets from this cluster in a prompt to the AI and have it provide an analysis of the common theme in the cluster.  Print out the analysis using the `fill` function so it is easy to read.
"""



"""## (8 points) 6. Generate Tweets from High Engagement Cluster with AI

Use the AI to generate three tweets that reflects the style and topic of the highest engagement cluster's tweets.  Put 10 tweets from the cluster in the `instructions` and have the `prompt` tell the AI to generate the tweet similar to these samples.  Print out the three generated tweets, making sure there is spacing and demarcation between them.
"""



"""# Problem 3. (13 points) Customer Service Tweets

## (2 points) 1. Load Data

Load the data in `"data/tweet_complaints_AmericanAir.csv"` into a dataframe `df_complaints`.  Print the head of the dataframe.
"""



"""## (4 points) 2. Customer Service Instructions

Create a string `instructions` that tells the AI to address the customer complaints. Create another string `instructions_style` that has information on the identity and style of the AI (name, background, speaking style, etc.  ).  Please be creative with the AI identity.  Add `instructions_style` to `instructions`. Print out `instructions` with the `fill` function.
"""



"""## (7 points) 3. Reply to Complaints with AI

Use `instructions` and the AI to generate a tweet to reply to each complaint in `df_complaints`.  You can put the complaint in the `prompt`.  Print the complaint tweet and the AI's response tweet below it.  Make sure you use the `fill` function and also put some space or new lines between each complaint/response pair so it is easy to read.  
"""

